The class_assignment file contains functonalities of 4 kinds of Stemmers.

1) Porter Stemmer
2) RegExp Stemmer
3) Lancaster Stemmer
4) Snowball Stemmer

Different words have been passed through all the 4 stemmers to see the output.
It was done to check how the stemmers react and respond to different suffixes and prefixes by providing us the root/stem words.

Another task that was performed is Lemmatization. Lemmatization usually refers to doing things properly with the use of a vocabulary
and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form 
of a word, which is known as the lemma.

A small task at the end was also performed of Vectorization. Vectorization is a methodology in NLP to map words or phrases from 
vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics. The process of converting words into numbers are called Vectorization.
We took 4 documents, calling them a corpus, and performed vectorization to see the numbers associating to that particular word.
